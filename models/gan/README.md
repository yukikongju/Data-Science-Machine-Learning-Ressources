# GAN Notes

There are 2 importants parts to understanding how GAN works:
- Architecture: we define a discriminator and a generator
- Training: unlike classic other models, we need to update the 
  weight of the discriminator and generator within the same loop

Training a GAN is equivalent to the following:
- For discriminator: Error(D(x), 1) + Error(D(G(z)), 0) 
- For the generator: Error(D(G(z)), 1)

Intuition:
- G(z) is the fake image generated by the discriminator
- D(x) is the classification made by the discriminator on real images
- D(G(z)) is the classification made by the discriminator on fake images
- The discriminator wants to distinguish the real images
  Error(D(x), 1) and the fake image Error(D(G(z)), 0)
- The generator wants the discriminator to classify its fake images 
  as real ie Error(D(G(z)), 1)

**Architecture - Discriminator**

The discriminator is a binary classifier that determines whether an 
image is fake or not. What is inside of it can be anything (linear, 
convolution layers, ...), what's important is that the last layer 
is of shape 1x1 with a sigmoid.

**Architecture - Generator**

The generator produces a fake image from a noise vector.


**Training**

We train the discriminator first and then the generator inside a 
single training step

1. Generate fake image from generator => `fake = generator(noise)`
2. Compute discriminator loss + update
    a. `lossD_real = criterion(discriminator(images), torch.ones_like(images))`
    b. `lossD_fake = criterion(discriminator(images), torch.zeros_like(fake))`
    c. `lossD = (lossD_real _ lossD_fake) / 2`
3. Compute generator loss + update
    a. `lossG = criterion(discriminator(fake), torch.ones_like(fake))`


## Docs

- [Jake Tae - GAN Maths](https://jaketae.github.io/study/gan-math/)

