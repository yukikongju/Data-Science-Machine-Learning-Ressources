{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b65e9a8d-c43c-402f-bc51-2e74ab3c672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b2f32-597a-4860-bc00-4f3472cc8b97",
   "metadata": {},
   "source": [
    "### 1. Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33948d4c-b8b0-4a9a-b400-5e7e1403cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"names.txt\", 'r') as f:\n",
    "    names = f.readlines()\n",
    "words = [name.strip() for name in names]\n",
    "num_words = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecb6f445-4189-44e4-9fb6-c60e1cebc932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary\n",
    "chars = '.' + 'abcdefghijklmnopqrstuvwxyz'\n",
    "itoc = {i: c for i, c in enumerate(chars)}\n",
    "ctoi = {c: i for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7d089c0f-278e-4892-a33a-42f108b8af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, block_size=3):\n",
    "    X, y = [], []\n",
    "    for word in words:\n",
    "        context = [0] * block_size\n",
    "        for c in word + '.':\n",
    "            idx = ctoi[c]\n",
    "            X.append(context)\n",
    "            y.append(idx)\n",
    "            context = context[1:] + [idx]\n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "    return X, y\n",
    "\n",
    "block_size = 8\n",
    "X, y = build_dataset(words=words, block_size=block_size)\n",
    "\n",
    "# split into training, validation and testing set\n",
    "n1, n2 = int(0.8 * num_words), int(0.9 * num_words)\n",
    "X_train, y_train = X[:n1], y[:n1]\n",
    "X_val , y_val = X[n1:n2], y[n1:n2]\n",
    "X_train, y_train = X[n2:], y[n2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e109591c-a414-4d8b-8ab9-53592cd77b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057368a8-c32e-4658-8a57-42e472ee7b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a68b537c-b09b-4bb3-9f9d-5e7f8ad3231a",
   "metadata": {},
   "source": [
    "### 2. Define the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e7b5c096-b776-4e79-b79c-f19a5073f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"\n",
    "    L = AX+B\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features_in, features_out, bias=True):\n",
    "        self.weight = torch.randn((features_in, features_out)) / features_in**0.5\n",
    "        self.bias = torch.zeros(features_out) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "        \n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class BatchNorm1D:\n",
    "\n",
    "    \"\"\"\n",
    "    https://en.wikipedia.org/wiki/Batch_normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # backprop params \n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # buffers params for momentum update\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # compute xmean and xvar\n",
    "        if self.training:\n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0,1)\n",
    "            xmean = x.mean(dim, keepdim=True)\n",
    "            xvar = x.var(dim, unbiased=False, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        # update the buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = self.momentum * xmean + (1 - self.momentum) * self.running_mean\n",
    "                self.running_var = self.momentum * xvar + (1 - self.momentum) * self.running_var\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class Tanh:\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class Embedding:\n",
    "  \n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    \n",
    "    def __call__(self, IX):\n",
    "        self.out = self.weight[IX]\n",
    "        return self.out\n",
    "  \n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class Sequential:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class FlattenConsecutive:\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T//self.n, C*self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d0f436-7546-4a39-9c52-4c465989bf87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a794054d-c706-4fae-833d-eb4d541ede8b",
   "metadata": {},
   "source": [
    "### 3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a414f9ea-d03f-4561-bfb0-e71d61eea7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "vocab_size = len(chars)\n",
    "emb_dim = 24 # dim of character embedding\n",
    "n_hidden = 128 \n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, emb_dim), \n",
    "    FlattenConsecutive(2), Linear(emb_dim * 2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size),\n",
    "])\n",
    "\n",
    "# params init\n",
    "with torch.no_grad():\n",
    "    model.layers[-1].weight *= 0.1\n",
    "\n",
    "parameters = model.parameters()\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0ac0efc1-0e32-4259-a8de-d509cd17a1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 2.3597769737243652\n",
      "Epoch 10001: 1.9274364709854126\n",
      "Epoch 20001: 2.132906436920166\n",
      "Epoch 30001: 1.6108429431915283\n",
      "Epoch 40001: 1.511803388595581\n",
      "Epoch 50001: 1.630881667137146\n",
      "Epoch 60001: 1.9283009767532349\n",
      "Epoch 70001: 1.7336162328720093\n",
      "Epoch 80001: 1.6993306875228882\n",
      "Epoch 90001: 1.8537205457687378\n",
      "Epoch 100001: 1.5659072399139404\n",
      "Epoch 110001: 1.9198318719863892\n",
      "Epoch 120001: 1.9517216682434082\n",
      "Epoch 130001: 1.4510877132415771\n",
      "Epoch 140001: 1.8526337146759033\n",
      "Epoch 150001: 1.7783403396606445\n",
      "Epoch 160001: 1.7433059215545654\n",
      "Epoch 170001: 1.7774038314819336\n",
      "Epoch 180001: 1.8917286396026611\n",
      "Epoch 190001: 1.5836371183395386\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "num_epochs = 200000\n",
    "mini_batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    # create mini-batch\n",
    "    idxs = torch.randint(low=0, high=X_train.shape[0], size=(mini_batch_size,))\n",
    "    Xb, Yb = X_train[idxs], y_train[idxs]\n",
    "\n",
    "    # forward pass \n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    lossi.append(loss)\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    lr = 0.1 if i < 15000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # show stats\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"Epoch {i+1}: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a51984-787b-41b9-a5c3-bff09340ba29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404488d-9e64-4fb0-8528-37136a3c473f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7c94336d-2bfb-43bd-9866-274fa4fec46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 24])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Embedding(vocab_size, emb_dim)\n",
    "p0 = X_train[idxs]\n",
    "t1 = t(p0)\n",
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6e08e529-0df2-4ab9-b4a7-16ffc39dd6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 48])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = FlattenConsecutive(2)\n",
    "t3 = t2(t1)\n",
    "t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d1b15419-ed8d-4267-b328-cc365e1c0631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 128])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4 = Linear(emb_dim * 2, n_hidden, bias=False)\n",
    "t4.parameters()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81ce8f9-18c6-45f6-b822-d2ccb8e51c4a",
   "metadata": {},
   "source": [
    "#### visualize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a9b3d-8a6e-4b62-9be2-6f51843b7ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16a70208-da3e-4bec-846c-5fc654375c19",
   "metadata": {},
   "source": [
    "### 4. Evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b2ff3-abb3-468e-9890-79bd87a31e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42b22c-8dcf-47ed-b3f4-05b074929e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdfb107-bc29-466c-9d4d-4324bba41c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed7de7d5-856b-49c8-a928-7e8548564be7",
   "metadata": {},
   "source": [
    "### 5. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5e7781d6-3182-4c1a-88b7-0a60c183efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word(model):\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass the neural net\n",
    "      logits = model(torch.tensor([context]))\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "      ix = torch.multinomial(probs, num_samples=1).item()\n",
    "      # shift the context window and track the samples\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      # if we sample the special '.' token, break\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    return ''.join(itos[i] for i in out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e6e37dc9-43e6-4fa3-b22f-275678529b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_word(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e2196106-9760-459f-bf9b-006fdc2614b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = [0] * block_size\n",
    "# model(torch.tensor([context]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b2307a3a-d6b5-422c-a3bf-02aed4774180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 128])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.layers[3].running_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0e0ab-c9a6-4073-becf-27daca0403f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
