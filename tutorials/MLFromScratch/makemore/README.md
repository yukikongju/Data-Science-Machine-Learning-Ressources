# makemore

Building the following languages models:
- [ ] Bigrams
- [ ] MLP
- [ ] RNN
- [ ] Transformers

**Prerequisites**

- Torch Generator
- Tensor Broadcasting
- Negative log Likelihood nv normalized neg log likelihood
- model smoothing to avoid zerero division
- one-hot encoding

Running the notebook: `python3 -m notebook`

## Bag of words

Steps:
- [ ] Probability dict of Bigrams as Tensor
- [ ] Using multinomial to sample from the model
- [ ] Basic Neural Neto with one layer

- Resource: [Andrej Kaparthy - The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2&ab_channel=AndrejKarpathy)

# Resources



