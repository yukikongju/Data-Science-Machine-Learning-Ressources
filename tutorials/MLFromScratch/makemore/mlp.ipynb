{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e62bbf64-d4cb-480c-9465-226633c9ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02150ed7-d40a-4323-907b-109b17be4dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14829284-9752-477a-923c-435855d88a0d",
   "metadata": {},
   "source": [
    "### 1. Build Dataset based on block size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "281a6dc5-2bcd-4f41-8517-a20b2fe92a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read names\n",
    "with open('names.txt', 'r') as f:\n",
    "    names = f.readlines()\n",
    "names = [name.strip() for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "730d3e22-9034-448b-a388-d0aa1f504afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build chars to int mapping\n",
    "chars = '.abcdefghijklmnopqrstuvwxyz'\n",
    "ctoi = {c: i for i, c in enumerate(chars)}\n",
    "itoc = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "574f6d3c-1532-4c1f-a9cf-f3e51425a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build dataset based on block size ie how many chars do we take to predict the next one\n",
    "def build_dataset(names: [str], block_size: int = 3):\n",
    "    X, Y = [], []\n",
    "    for name in names:\n",
    "        context = [0] * block_size\n",
    "        for ch in list(name) + ['.']:\n",
    "            idx = ctoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(idx)\n",
    "            context = context[1:] + [idx]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca5758a7-ec19-420a-81b5-ae9ff522d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- split into training and testing set\n",
    "\n",
    "# creating dataset\n",
    "X, Y = build_dataset(names=names, block_size=3)\n",
    "\n",
    "# generating random indexes\n",
    "indexes = list(range(X.shape[0]))\n",
    "random.shuffle(indexes)\n",
    "train_perc = 0.8\n",
    "train_size = int(X.shape[0] * train_perc)\n",
    "\n",
    "# spliting \n",
    "X_train, Y_train = X[:train_size], Y[:train_size]\n",
    "X_test, Y_test = X[train_size:], Y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e68e7ed1-8b12-46b8-a1b0-0481e4bc83c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182516, 3]) torch.Size([182516])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d210d8-5fcc-4520-b8f1-f3c93afd5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build chars-int dictionary: mapping from subsequence of chars to int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2001115a-e5d0-4bfd-bb51-e7371a3b6b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c945d22c-849a-4970-859a-3e29d0869084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode with one-hot encoding (?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e8094e-b0b5-4f6f-b214-ba9e502b9cde",
   "metadata": {},
   "source": [
    "### 2. Build and Train Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97dfeeaf-274d-4fe6-99ef-fcf1a213cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize embedding -> each character is represented by a vector of {vector_size}\n",
    "# initialize MLP layers -> we have two layers\n",
    "# layer 1: dim W1: (vector_size * block_size, t1)\n",
    "# layer 2: dim W2: (1, 27) ie 27: num of chars\n",
    "vector_size = 10\n",
    "C = torch.randn((27, vector_size))\n",
    "embedding = C[X] # shape: [228146, 3, 2]\n",
    "W1 = torch.randn((30, 200))\n",
    "b1 = torch.randn(200)\n",
    "W2 = torch.randn((200, 27))\n",
    "b2 = torch.randn(27)\n",
    "params = [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b63264a3-3e3d-47f0-90c1-b8e42529f2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11627"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num of parameters in total\n",
    "sum(p.nelement() for p in params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c14488ce-64cb-46c6-83b0-19c149dd7a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters to training mode\n",
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f15a1-7d01-48e1-bb3d-171789332982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "75220311-4f48-482b-a31f-7e6b417e4ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-batch gradient descent + learning rate decay\n",
    "num_epochs = 200000\n",
    "n = X_train.shape[0]\n",
    "\n",
    "losses = []\n",
    "for i in range(num_epochs):\n",
    "    # indexing mini-batch\n",
    "    indexes = torch.randint(low=0, high=n, size=(32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X_train[indexes]]\n",
    "    h = torch.tanh(emb.view(-1, 30) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = torch.cross_entropy(logits, Y_train[indexes])\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 150000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += lr * p.grad\n",
    "\n",
    "    # print loss\n",
    "    losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9295ee0-8343-4148-ba09-de628ca5b8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16c08a80-a7c5-477a-9e54-4001385f9f21",
   "metadata": {},
   "source": [
    "### 3. Visualize Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a1442-9546-4a17-b5ed-bb10cc518d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396dee9-2a4e-4ace-a81c-07ec6f63649d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e041085-eae5-4313-8353-08a8e5b53dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9965ee-29ca-42d8-b8f9-937dc4986c35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
