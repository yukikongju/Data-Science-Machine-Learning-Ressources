{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction Using Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholding Numerical Feature Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 1.4, 0.2],\n",
       "       [4.9, 1.4, 0.2],\n",
       "       [4.7, 1.3, 0.2]])"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "tragets= iris.target\n",
    "\n",
    "# create thresholder\n",
    "thresholder = VarianceThreshold(threshold = .5)\n",
    "\n",
    "# create features matrix\n",
    "feature_mat = thresholder.fit_transform(features)\n",
    "\n",
    "# view mat\n",
    "feature_mat[:3]\n",
    "\n",
    "# view variance\n",
    "thresholder.fit(features).variances_\n",
    "\n",
    "# premisse: features with low variance are less interesting than \n",
    "# feature with high variance. formula: moyenne (value- mean) squared\n",
    "# we can't use  standardizer because all values will be 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholding with Binary Feature variacne:select subset of features with bernouilli random var above threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# create features matrix\n",
    "features = [[0, 1, 0],\n",
    " [0, 1, 1],\n",
    " [0, 1, 0],\n",
    " [0, 1, 1],\n",
    " [1, 0, 0]]\n",
    "\n",
    "# create threshold\n",
    "threshold = VarianceThreshold(threshold = .75 * (1 - .75))\n",
    "\n",
    "# subset variables\n",
    "threshold.fit_transform(features)\n",
    "\n",
    "# we examine a ds variacne with bernouilli random var: p(1-p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove highly correlated features with correlatioin matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  2\n",
       "0  1  1\n",
       "1  2  0\n",
       "2  3  1\n",
       "3  4  0\n",
       "4  5  1"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create data with highly correlated values\n",
    "features = np.array([[1, 1, 1],\n",
    " [2, 2, 0],\n",
    " [3, 3, 1],\n",
    " [4, 4, 0],\n",
    " [5, 5, 1],\n",
    " [6, 6, 0],\n",
    " [7, 7, 1],\n",
    " [8, 7, 0],\n",
    " [9, 7, 1]])\n",
    "\n",
    "# convert features into matrix\n",
    "df = pd.DataFrame(features)\n",
    "\n",
    "# create correlation matrix\n",
    "corr_matrix = df.corr().abs()\n",
    "\n",
    "# determine variables to keep: upper triangle\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),\n",
    " k=1).astype(np.bool))\n",
    "\n",
    "# find index of features to drop: remove feat above 95 of correlation\n",
    "col_indices_to_drop = [column for column in upper.columns\n",
    "                       if any(upper[column] > 0.95)]\n",
    "\n",
    "# drop columns\n",
    "df.drop(df.columns[col_indices_to_drop], axis =1). head()\n",
    "\n",
    "## Note: we use corr() to view higl correlated variables,\n",
    "# and remove all except one because the information we get\n",
    "# is redundant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Irrelevant Features for Classification with chi-square statistic and SelectKbest for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "\n",
    "# load data\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "targets = iris.target\n",
    "\n",
    "# Convert categorical data in integer\n",
    "features = features.astype(int)\n",
    "\n",
    "### Select the best two features: Method with highest chi-square\n",
    "# if values are categorical\n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "features_kbest = chi2_selector.fit_transform(features, targets)\n",
    "\n",
    "### Select the best two features: Method with f_class (ANOVA) for\n",
    "# values are quantitative\n",
    "fvalue_selector = SelectKBest(f_classif, k=2)\n",
    "features_kbest = fvalue_selector.fit_transform(features, targets)\n",
    "\n",
    "# Select the best n features using percentiles\n",
    "fvalue_selector = SelectPercentile(f_classif, percentile = 75)\n",
    "features_kbest = fvalue_selector.fit_transform(features, targets)\n",
    "\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kbest.shape[1])\n",
    "\n",
    "## Note: we want to remove uninformaive features. \n",
    "# we apply chi-square (X-squared) stat between each feature \n",
    "# and target vector to determine independance between nb of \n",
    "# obs and class\n",
    "### if two variables are higly dependant, they are informative\n",
    "# when training the model\n",
    "### ANOVA with f_classif calculate mean for each class and tells\n",
    "# if they are different\n",
    "### Select Percentile is an alternative \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Irrelevant Features for Classification with f_classif ANOVA and SelectPercentile for quantitative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "targets = iris.target\n",
    "\n",
    "# Convert categorical data in integer\n",
    "features = features.astype(int)\n",
    "\n",
    "### Select the best two features: Method with f_class (ANOVA) for\n",
    "# values are quantitative\n",
    "fvalue_selector = SelectKBest(f_classif, k=2)\n",
    "features_kbest = fvalue_selector.fit_transform(features, targets)\n",
    "\n",
    "# Select the best n features using percentiles\n",
    "fvalue_selector = SelectPercentile(f_classif, percentile = 75)\n",
    "features_kbest = fvalue_selector.fit_transform(features, targets)\n",
    "\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kbest.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursively remove features with RFECV: recursive feature elimination with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.39766312,  0.35157785],\n",
       "       [-1.34432116, -0.16255151],\n",
       "       [ 0.11539519, -0.91775481],\n",
       "       ...,\n",
       "       [-0.50487478, -0.07594816],\n",
       "       [ 0.12172277,  1.98293771],\n",
       "       [-0.04197372,  1.16751964]])"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# Suppress an annoying but harmless warning\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\",\n",
    " message=\"^internal gelsd\")\n",
    "\n",
    "# generate features matrix and target vector with make regression\n",
    "features, target = make_regression( n_samples = 10000,\n",
    "                                  n_features = 100,\n",
    "                                  n_informative =2, \n",
    "                                  random_state = 420)\n",
    "\n",
    "# create linear regression\n",
    "lin_reg = linear_model.LinearRegression()\n",
    "\n",
    "# recursively remove features\n",
    "rfecv = RFECV(estimator = lin_reg, step = 1,\n",
    "              scoring = 'neg_mean_squared_error')\n",
    "rfecv.fit(features, target).transform(features)\n",
    "\n",
    "### WE train a model and remove its worst feature until the\n",
    "# model accuracy becomes worse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
