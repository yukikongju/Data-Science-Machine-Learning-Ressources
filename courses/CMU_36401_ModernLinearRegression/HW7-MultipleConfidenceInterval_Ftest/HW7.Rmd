---
title: "HW7 -"
author: "Emulie Chhor"
date: "30/12/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(MASS)
library(curl)
library(dplyr)
library(ggplot2)
library(GGally)

```

# Question 1 - 

```{r}
sports <- read.table("~/Documents/OneDrive/Bac-Maths-Info/Winter2022/ModernRegression_36-401/datasets/sports.txt", sep = ' ', header = TRUE)
names(sports)
summary(sports)
df <- sports %>% 
  select(-c("Label", "Sport"))
```

## a) Use pairs to plot the data. Comment any patterns you see.

```{r}
ggpairs(df, alpha = 0.2, color = 'seagreen')
pairs(df)
```
The following variables have a positive correlation somewhat importants:
- Ht: Wt and LBM
- Wt: LBM, BMI
- LBM: BMI
- RCC: Hc, Hg
- WCC: Nonne
- Hc: Hg
- Ferr: None
- CMI: None
- SFF: BFat

## b) Use a linear regression model to predict LBM from Sex, Ht, Wt, RCC, WCC, Hc, Hg, Ferr, BMI, Bfat. Comment on your residual plots

```{r}
model1 <- lm(LBM~Sex+Ht+Wt+RCC+WCC+Hc+Hg+Ferr+BMI+Bfat, data = sports)
model2 <- lm(LBM~. , data = df)

plot(fitted(model1), residuals(model1), pch=19, col=alpha('seagreen', 0.6))
abline(h=mean(residuals(model1)), col='seagreen')
```

By plotting the residuals plot, it seems that some of the linear regression 
assumptions aren't met. We see that the residuals mean is around 0, but the 
homoscedacity assumption is violated: the variance isn't constant.

## c) Summarize your fitted model

```{r}
summary(model1)$coefficients
```
By looking at the p-value, it seems that the most important variables in the model are Wt and BFat.



## d) Find the eigenvalues of the design matrix

Rappel: 
- design matrix: the dataset without the response variable (ie the features X)
- eigenvalues: we solve $det(A-\lambda u) = 0$

WHY 

```{r}
X <- sports %>% 
  select(-c("Label", "Sport", "LBM")) %>% 
  as.matrix()
# X <- as.matrix(sports[,c(1:3, 5:11)])

eigen_values <-  eigen(t(X) %*% X)$values
  
barplot(eigen_values, col='seagreen')
```


## e) Construct a 90 percent confidence rectangle for all the coefficients in the model (except intercept)



```{r}
kable(confint(model1, level = 0.99, parm = 2:11), digits = 2)
```


```{r}

# made with ChatGPT

# Extract the variance-covariance matrix of the model
vcov_matrix <- vcov(model1)

# Compute the eigenvalues and eigenvectors of the variance-covariance matrix
eigen <- eigen(vcov_matrix)

# Define the confidence level (e.g. 95%)
confidence_level <- 0.95

# Compute the critical value from a chi-square distribution
critical_value <- qchisq(confidence_level, df = 2)

# Define the semi-axes of the rectangle as the square root of the eigenvalues
semi_axes <- sqrt(eigen$values)

# Plot the rectangle
plot(0, 0, xlim = c(-semi_axes[1]*critical_value, semi_axes[1]*critical_value),
     ylim = c(-semi_axes[2]*critical_value, semi_axes[2]*critical_value),
     xlab = "beta1", ylab = "beta2",
     main = paste("Confidence interval rectangle (", confidence_level*100, "%)", sep = ""))

# Add the rectangle to the plot
rect(-semi_axes[1]*critical_value, -semi_axes[2]*critical_value,
     semi_axes[1]*critical_value, semi_axes[2]*critical_value, col = "grey80")
```



## f) Fit a linear regression to predict LBM from Sex, Ht, Wt and RCC. Summarize the fitted model


```{r}
model2 <- lm(LBM ~ Sex + Ht + Wt + RCC, data = sports)
summary(model2)$coefficients
```



## g) Construct and plot a 95 percent confidence ellipsoid for Ht and Wt


```{r}

```


## h) Construct an F test to compare the two models that you fit. Summarize and interpret the results of the test

The F test is a statistical test used to compare the goodness of fit of two statistical models. We first compute the F statistic for each model and select the model with the highest value. 

The F statistic give us the goodness of fit of the model. It can be computed as follows:
- SSR: sum of squared residuals -> $\sum{(y - \hat{y})^2}$
- MSE: mean squared error: average between observed and predicted response -> $MSE = \frac{SSR}{n-p-1}$, where n: number of observations, p: number of parameters 
- ESS: Explained sum squared -> $ESS=\sum{(y - \bar{y})^2}$

$F=ESS/MSE$

If the F-statistic is 0, it means that the model can't be used to predict the response variable.


 F statistic for model 1
```{r}
n <- nrow(sports)
p <- length(X)
X <- sports %>% 
  select(-c("LBM", "Label", "Sport"))
Y <- sports$LBM
SSR1 <- sum((Y - predict(model1, X))^2)
MSE1 <- SSR1 / (n-p-1)
ESS1 <- sum((Y- mean(Y))^2)
F1 <- ESS1/MSE1
```


F statistic for model 2
```{r}
X2 <- sports %>% 
  select(c("Sex", "Ht", "Wt", "RCC"))
n <- nrow(sports)
p2 <- length(model2$coefficients)
SSR2 <- sum((Y-predict(model2, X2))^2)
MSE2 <- SSR2 / (n-p2-1)
ESS2 <- sum((Y-mean(Y))^2)
F2 <- ESS2/MSE2
```


Using anova
```{r}
kable(anova(model2, model1), caption = "Analysis of Variance")

```

We see that the F-statistic for the first model is larger than the for the model2, so we conclude that model1 encaptures more information than model2. 

I'm not sure why anova() method in R generates NA though...


