{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction Using Feature Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing Features Using Principal Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original features numb: 64\n",
      "pca on df 40\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "\n",
    "# load data\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Standardize the feature matrix\n",
    "features = StandardScaler().fit_transform(digits.data)\n",
    "\n",
    "# Create PCA\n",
    "pca = PCA(n_components = 0.95, whiten = True)\n",
    "\n",
    "# fit PCA\n",
    "features_pca = pca.fit_transform(features)\n",
    "\n",
    "# Show results\n",
    "print(\"original features numb:\", features.shape[1])\n",
    "print('pca on df', features_pca.shape[1])\n",
    "\n",
    "# we use n_components between 0.95 and 0.99 to use 95% to 99% \n",
    "# of variance retained\n",
    "# Whiten: make mean = 0 and unit variance\n",
    "# svd_solver ='randomized' : stochastic algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing Feature when Data is Linearly Inseparable using Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data shape: (1000, 2)\n",
      "kernel pca : (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# generate inseparable data\n",
    "features, _ = make_circles(n_samples = 1000,\n",
    "                          random_state = 420,\n",
    "                          noise = 0.1,\n",
    "                          factor = 0.1)\n",
    "\n",
    "# create kernel pca\n",
    "kernel_pca = KernelPCA(kernel = 'rbf', gamma =15, n_components = 1)\n",
    "\n",
    "# reduce features\n",
    "features_pca = kernel_pca.fit_transform(features)\n",
    "\n",
    "# view\n",
    "print('original data shape:', features.shape)\n",
    "print('kernel pca :', features_pca.shape)\n",
    "\n",
    "# note: pca tries to maximize variance\n",
    "\n",
    "# we use rbf as kernel for gaussian radical basis function\n",
    "# using kernel = 'linear' makes regular PCA\n",
    "# n_components: choose the number of parameters\n",
    "# gamma : kernel hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing Featrues by Maximizing Class Separability with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original df :  (150, 4)\n",
      "features lda:  (150, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.9912126])"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# create lda\n",
    "lda = LinearDiscriminantAnalysis(n_components = 1)\n",
    "\n",
    "# reduce features\n",
    "features_lda = lda.fit(features, target).transform(features)\n",
    "\n",
    "# view\n",
    "print('original df : ', features.shape)\n",
    "print('features lda: ', features_lda.shape)\n",
    "\n",
    "# amount of variance explained by eacg component\n",
    "lda.explained_variance_ratio_\n",
    "\n",
    "# note: lda tries to maximize mean of clusters and minimize \n",
    "# variance for better separation. has additional goal than pca:mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing Features with non negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 64\n",
      "Reduced number of features: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn import datasets\n",
    "\n",
    "# load data\n",
    "digits = datasets.load_digits()\n",
    "features = digits.data\n",
    "\n",
    "# create NMF\n",
    "nmf = NMF(n_components = 10, random_state =420)\n",
    "\n",
    "# reduce features\n",
    "features_nmf = nmf.fit_transform(features)\n",
    "\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_nmf.shape[1])\n",
    "\n",
    "# unsupervised technique for linear dimensionality reduction\n",
    "# breaks up data into latent variabels.\n",
    "# values has to be positive to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing Features using Sparse Data using TSVD: only non nul elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features standardized: 64\n",
      "sparse matrix features: 64\n",
      "Reduced number of features: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5887245646379315"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# standardize features\n",
    "features_standardized = StandardScaler().fit_transform(digits.data)\n",
    "\n",
    "# put features in sparse matrix\n",
    "sparse_mat = csr_matrix(features_standardized)\n",
    "\n",
    "# create TSVD\n",
    "tsvd = TruncatedSVD(n_components = 10)\n",
    "\n",
    "# apply tsvd on sparse mat\n",
    "features_sparse_tsvd = tsvd.fit_transform(sparse_mat)\n",
    "\n",
    "# Show results\n",
    "print(\"features standardized:\", features_standardized.shape[1])\n",
    "print(\"sparse matrix features:\", sparse_mat.shape[1])\n",
    "print(\"Reduced number of features:\", features_sparse_tsvd.shape[1])\n",
    "\n",
    "# see data loss\n",
    "tsvd.explained_variance_ratio_[:].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
