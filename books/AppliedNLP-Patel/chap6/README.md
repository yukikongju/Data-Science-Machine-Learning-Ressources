# Chap 6 - Recurrent Neural Networks and Other Sequence Models

[Code](https://github.com/nlpbook/nlpbook/blob/main/ch06.ipynb)


- [o] Sequence Modeling using
    - [X] Dummy RNN (no embedding)
    - [X] Reccurent Neural Neworks (RNNs)
    - [X] RNN Embedding flatten (?)
    - [ ] Bidirectionals RNNs
    - [ ] Long-Short Term Memory (LSTMs)
    - [ ] Gated Recurrent Units (GRUs)
- [ ] More Models
    - [ ] AWD-LSTMs
    - [ ] QRNNs
    - [ ] SHA-RNNs

**To Research**
- [ ] Why is there a difference between RNNEmbeding and RNNEmbeddingFlatten?


**Notes**



## Ressources


